# GenVidBench: A Challenging Benchmark for Detecting AI-Generated Video


## Instruction
---
The rapid advancement of video generation models has made it increasingly challenging to distinguish AI-generated videos from real ones. This issue underscores the urgent need for effective AI-generated video detectors to prevent the dissemination of false information through such videos. However, the development of high-performance generative video detectors is currently impeded by the lack of large-scale, high-quality datasets specifically designed for generative video detection. To this end, we introduce GenVidBench, a challenging AI-generated video detection dataset with several key advantages: 1) Cross Source and Cross Generator: The cross-generation source mitigates the interference of video content on the detection. The cross-generator ensures diversity in video attributes between the training and test sets, preventing them from being overly similar. 2) State-of-the-Art Video Generators: The dataset includes videos from 8 state-of-the-art AI video generators, ensuring that it covers the latest advancements in the field of video generation. 3) Rich Semantics: The videos in GenVidBench are analyzed from multiple dimensions and classified into various semantic categories based on their content. This classification ensures that the dataset is not only large but also diverse, aiding in the development of more generalized and effective detection models. We conduct a comprehensive evaluation of different advanced video generators and present a challenging settings.

## FIT5230 新增：
### 论文解读
对应的论文如下：[sample paper.pdf](https://github.com/user-attachments/files/22000697/sample.paper.pdf)

在GenVidBench数据集中，训练集和测试集的划分严格遵循**跨来源（Cross Source）和跨生成器（Cross Generator）** 原则，目的是模拟真实场景中检测模型面对未知生成来源和生成器时的挑战。

<img width="611" height="291" alt="video_source" src="https://github.com/user-attachments/assets/75b0a8c9-8d0e-44ed-91d6-1f8abdd3a73d" />

#### **1. 训练集：来自“其他来源”的视频对**
训练集由与测试集**不同来源**且使用**不同生成器**生成的视频组成，避免模型依赖特定内容或生成器的属性。  
**示例**：  
- 训练集主要包含“Pair1”中的视频，real视频为ript，fake视频由4个文本到视频（T2V）生成器基于**VidProM数据集的文本提示**生成：  
  - Pika  
  - VideoCrafter2  
  - Modelscope
  - Text2Video-Zero  
  每个生成器贡献13,500个视频，内容均匹配VidProM的文本提示（如“一群人在山顶朝拜佛像”）。  

#### **2. 测试集：与真实视频“同来源”的视频对**
测试集由与真实视频**相同来源**但使用**不同生成器**生成的视频组成，且生成器与训练集完全不同，增加检测难度。  
**示例**：  
- 测试集主要包含“Pair2”中的视频，这些视频与真实视频来源（HD-VG-130M数据集）一致：  
  - 基于HD-VG-130M的**图像帧**生成：MuseV（I2V，2024.03，1210×576，12 FPS）、SVD（I2V，2023.11，1024×576，10 FPS）。  
  - 基于HD-VG-130M的**文本提示**生成：Mora（T2V，2024.03，1024×576，10 FPS）、CogVideo（T2V，2022.05，480×480，3 FPS）。  
  每个生成器贡献13,800个视频，内容与HD-VG-130M的真实视频匹配（如“厨房中坐在椅子上的男孩”）。

*VidProM是一个用于文本到视频扩散模型的大规模数据集，它为Pika、VideoCrafter2、Modelscope、T2V-Zero提供数据支持，这些模型基于VidProM的文本提示生成视频，从而参与构成GenVidBench数据集。*

#### **核心区别与挑战**
| 维度         | 训练集（Pair1）                          | 测试集（Pair2）                          |
|--------------|-----------------------------------------|-----------------------------------------|
| **来源**     | 基于VidProM的文本提示                    | 基于HD-VG-130M的图像/文本（与真实视频同来源） |
| **生成器**   | Pika、VideoCrafter2等（与测试集无重叠）   | MuseV、SVD、Mora、CogVideo（与训练集无重叠） |
| **目的**     | 让模型学习通用检测特征，不依赖特定来源或生成器 | 测试模型对未知来源和生成器的泛化能力          |

例如：若训练集学习了Pika生成的“自然风景”视频特征，测试集则要求模型检测MuseV生成的同一场景（自然风景）视频——两者来源不同、生成器不同，但内容相似，因此模型无法仅通过内容或生成器特征区分真假，必须学习更本质的伪造痕迹。

#### 相较于其他检测器的功能
<img width="530" height="119" alt="Snipaste" src="https://github.com/user-attachments/assets/2b72ec98-bb94-4287-b3c4-e9a77c358dcb" />

#### 数据集中fake视频生成规则
<img width="372" height="172" alt="Snipaste" src="https://github.com/user-attachments/assets/060146fe-7861-4ad1-9c68-4a056039599d" />

生成视频的语义基于主要维度的类别分类提取。即，将描述分为三个关键层级：物体、动作和地点，因为这三个元素能够构成一个完整的句子或故事，且具有丰富的语义。

#### 模型探讨
1. 交叉与非交叉的成功率 (以videoSwinTiny模型为例）
    - 对非交叉（某模型生成并验证）：非常高
    <img width="556" height="197" alt="Snipaste_2025-08-27_13-54-32" src="https://github.com/user-attachments/assets/4a82e7a7-27b0-4de5-9867-e381e3f429fe" />

    - 针对交叉：明显下降
    <img width="516" height="149" alt="Snipaste_2025-08-27_14-04-13" src="https://github.com/user-attachments/assets/709a61f9-eea2-44f6-905e-254ca658cc71" />

2. 先进模型对我们数据集（子集）的成功率
<img width="580" height="241" alt="Snipaste_2025-08-27_14-29-08" src="https://github.com/user-attachments/assets/25890237-bd61-4ece-8b4c-a90735c89bd5" />

在跨来源和跨生成器任务上训练的各种最先进方法的结果。最优性能指标不超过79.90%，这表明该任务具有相当大的挑战性。

#### 数据集探讨
1. 一些模型针对先进数据集和我们组合数据集的成功率

<img width="372" height="193" alt="Snipaste_2025-08-27_14-32-32" src="https://github.com/user-attachments/assets/6fe366ab-96ae-4461-9639-ac45e96ec93f" />

这表明我们的数据集更具有挑战性。

2. 数据集内部疑难案例

 <img width="466" height="102" alt="Snipaste_2025-08-27_14-32-32" src="https://github.com/user-attachments/assets/d4afaf83-7729-474b-ab15-fd614d5b8ca3" />

不同生成器中不同类别上疑难案例的比例。植物类最容易被误分类，而卡通类最容易区分。

 <img width="424" height="116" alt="Snipaste_2025-08-27_14-45-46" src="https://github.com/user-attachments/assets/7a341024-3bdb-4e5a-bb15-b1ebea96d00b" />

如上图，基于对难例的分析，我们选择了植物类别，并给出了各种模型在该类别上的实验结果
图中显示，SVD的分类准确率最低，这证明其生成性能最佳。

上述实验结果表明，模型在单一场景中的分类性能与在所有场景中的分类性能存在差异，因此有必要针对不同场景进行视频检测生成。GenVidBench提供的丰富语义标签有助于分析每个场景。


### 代码框架
文中围绕**AI生成视频检测**的核心流程，从数据准备、预处理、模型训练、任务执行到性能评估，形成了一套完整的检测步骤，具体可拆解为以下5个阶段，每个阶段均有明确的设计逻辑与实验依据：


#### 一、数据准备阶段：构建检测所需的高质量数据集
检测的前提是获取符合“挑战性”需求的训练与测试数据，该阶段核心是基于**GenVidBench数据集**完成数据筛选、划分与标签匹配，具体步骤如下：
1. **数据集来源获取**  
   - 真实视频：从两个公开数据集提取——Vript（2024.04，1280×720分辨率，30 FPS，20131条）和HD-VG130M（2023.05，1280×720分辨率，30 FPS，13800条），确保真实视频的多样性与高分辨率。  
   - 伪造视频：覆盖8个主流AI视频生成器（含T2V和I2V类型），如Pika（2022.05，T2V）、MuseV（2024.03，I2V）、SVD（2023.11，I2V）等，每条生成器贡献13500-13800条视频，覆盖不同分辨率（256×256~1210×576）和帧率（3~24 FPS）（详见Table 1）。

2. **训练集与测试集划分（跨源+跨生成器原则）**  
   - 核心逻辑：避免训练与测试数据因“生成源（文本提示/图像）”或“生成器”相似导致检测偏差，确保泛化性。  
   - 具体划分：将数据分为2组视频对（Pair1和Pair2）：  
     - Pair1（T2V生成）：含Pika、VideoCrafter2等4个生成器，基于VidProM的统一文本提示生成；  
     - Pair2（I2V+T2V生成）：含MuseV（I2V）、SVD（I2V）、Mora（T2V）等4个生成器，基于HD-VG130M的图像/文本提示生成。  
   - 最终划分：**训练集=Pair1+Vript（真实视频）**，**测试集=Pair2+HD-VG130M（真实视频）**，确保训练与测试的“生成源”和“生成器”完全不同（3.4节）。

3. **语义标签匹配（用于特定场景检测）**  
   对所有视频按“物体（People/Animals等）、动作（Static Postures/Active Engagement等）、场景（Natural Landscapes/Transportation等）”三个维度标注语义标签（Fig. 2），支持后续“特定场景检测”（如植物类、车辆类视频的针对性检测）（3.2节）。


#### 二、数据预处理阶段：统一输入格式与增强鲁棒性
为确保不同来源的视频可被模型统一处理，需对输入数据进行标准化预处理，步骤如下（4.1节）：
1. **帧采样与尺寸调整**：从每个视频中采样8帧作为模型输入，将每帧图像 resize 至224×224（适配主流视频模型的输入要求）；  
2. **数据增强**：采用随机翻转（Horizontal Flip）、随机裁剪（Random Crop）等方法，减少模型过拟合；  
3. **训练配置统一**：使用MMAction2框架的默认配置，批量大小（Batch Size）设为8，学习率采用各模型在MMAction2中的默认值，确保实验可复现。


#### 三、检测模型选择与训练阶段：选用主流模型并标准化训练
检测的核心是通过模型学习“真实视频”与“AI生成视频”的差异特征，步骤如下：
1. **模型选择**：选取13种主流视频分类/检测模型，覆盖CNN和Transformer两大架构，包括：  
   - CNN类：SlowFast、I3D、X3D、TSM等；  
   - Transformer类：VideoSwin、UniFormer V2、MViT V2、TimeSformer等（4.2节）；  
   选择依据：这些模型在视频理解任务中表现优异，可作为AI生成视频检测的“基准模型”。

2. **模型训练**：基于MMAction2框架，使用上述预处理后的训练集进行训练，训练目标为“二分类任务（真实=0，伪造=1）”，优化器与迭代次数遵循MMAction2的默认配置，确保训练过程标准化。


#### 四、检测任务执行阶段：分场景验证模型性能
设计两类核心检测任务，全面评估模型的泛化能力与场景适应性：
1. **核心任务1：跨源跨生成器检测**  
   目标：验证模型在“未知生成源+未知生成器”场景下的检测能力（最贴近真实应用场景，因实际中无法预知AI视频的生成工具与提示）。  
   执行方式：用“训练集（Pair1+Vript）”训练模型后，在“测试集（Pair2+HD-VG130M）”上测试，输出模型对“真实视频”和“8个生成器的伪造视频”的分类结果（4.2节，Table 5）。

2. **核心任务2：特定场景检测**  
   目标：分析模型在不同语义场景下的检测性能（如植物类视频是否更难区分）。  
   执行方式：基于语义标签提取特定场景的子数据集（如“植物类”“车辆类”“卡通类”），用训练好的模型在子数据集上测试，评估场景对检测难度的影响（4.5节，Table 8）。


#### 五、性能评估与分析阶段：量化检测效果并定位难点
通过多维度评估验证检测效果，并分析“难检测案例”以指导后续优化，步骤如下：
1. **评估指标**：采用**Top1准确率**作为核心指标（即模型预测的最高概率类别与真实类别一致的比例），直观反映检测正确率；  
2. **模型性能对比**：  
   - 跨任务对比：在“跨源跨生成器任务”中，MViT V2表现最优（Top1=79.90%），SlowFast表现最差（Top1=41.66%），证明该任务的挑战性（Table 5）；  
   - 跨数据集对比：将模型在GenVidBench与其他数据集（如GVF、GenVideo、DeepFakes）上的性能对比，发现模型在GenVidBench上的准确率显著更低（如SlowFast在GVF上60.95% vs GenVidBench上41.66%），证明GenVidBench的挑战性更高（Table 6）；  
3. **硬案例分析（Hard Case Analysis）**：  
   定义“模型预测为‘真实视频’的概率<3%的AI生成视频”为硬案例，分析不同生成器和语义类别的硬案例比例：  
   - 生成器维度：SVD的硬案例比例最高（均值0.739），说明SVD生成的视频最难检测；CogVideo的硬案例比例最低（均值0.017），因其实时连续性差（Table 7）；  
   - 语义类别维度：“植物类（Plants）”是最难检测的类别（硬案例比例均值0.318），“卡通类（Cartoon）”最易检测（均值0.209）（Fig. 6）。


#### 总结：检测步骤的核心逻辑
文中的检测步骤围绕“**高挑战性数据→标准化预处理→泛化性模型→多场景验证→难点定位**”展开，核心目标是模拟真实世界中“未知生成源、未知生成器、多样场景”的检测需求，通过“跨源+跨生成器”的设计，确保检测模型具备强泛化能力，而非仅适配特定生成工具

### 任务解读
作为light方：
1. 提供一个框架，供dark方进行检测，最终获得结果（是否由AI生成）
2. 针对dark方原始生成模式以及改进后的模式持续优化，使得检测更准确。

基于GenVidBench项目，将其改造为**特定AI生成技术视频的检测**（如仅检测SVD生成的视频、仅检测I2V（图像到视频）类技术生成的视频等），需围绕“目标技术特性聚焦、数据集定向改造、模型针对性优化、评估体系适配”四大核心环节展开，具体处理步骤如下，均参考原项目设计逻辑并结合特定技术场景调整：


#### 一、第一步：明确“特定AI生成技术”的范围与核心特征
改造的前提是精准界定目标技术的边界，避免范围模糊导致后续工作偏移。需先完成以下分析，可参考原项目对8种生成器的特性拆解方法（3.3节、3.5节）：
1. **界定技术类型**：明确目标技术的核心属性，例如：
   - 按“生成范式”划分：仅针对T2V（文本到视频，如Pika、VideoCrafter2）或I2V（图像到视频，如MuseV、SVD）；
   - 按“特定生成器”划分：仅针对某一款生成器（如SVD、Mora，原项目中SVD生成的视频因质量高最难检测）；
   - 按“技术细节”划分：仅针对某类生成原理（如基于扩散模型的视频生成技术、基于GAN的视频生成技术）。
2. **提取目标技术的独特特征**：参考原项目对生成器“时空质量”的评估维度（3.3节，Fig.5），梳理目标技术的差异化特征，例如：
   - 时空属性：分辨率（如SVD为1024×576）、帧率（如CogVideo仅3 FPS）、视频时长（如Pika为3秒，其他多为1-2秒）；
   - 生成缺陷：是否存在特定 artifacts（如I2V技术可能因图像输入导致帧间一致性偏差，T2V技术可能因文本理解偏差导致物体形态异常）；
   - 语义偏好：是否在某类场景中表现更优（如SVD在“车辆类”视频生成中更逼真，原项目4.4节提到SVD的车辆类视频硬案例比例高）。
这些特征将直接指导后续数据集改造与模型优化方向。


#### 二、第二步：定向改造GenVidBench数据集，聚焦目标技术
原项目的核心优势是“跨源跨生成器、语义标签丰富”（摘要、3.1节），改造时需保留这一框架，但针对目标技术“收缩数据范围、强化针对性样本”，具体操作如下：
##### 1. 筛选/扩充“目标技术的AI生成视频”样本
- **样本筛选（若目标技术已在原数据集中）**：从原GenVidBench的8个生成器子集（Table 1）中，提取目标技术对应的所有样本。例如：若目标是检测SVD生成的视频，则提取原数据集中SVD的13800条I2V视频（Pair2，基于HD-VG130M图像生成）；
- **样本扩充（若需增加数据多样性）**：参考原项目“基于真实视频的提示生成伪造视频”的方法（3.1节、3.4节），补充目标技术的新样本：
  - 生成源匹配：若目标技术是I2V（如SVD），则从原真实视频源（Vript、HD-VG130M）中提取更多图像帧，用目标技术生成新的I2V视频；若目标技术是T2V（如Pika），则基于原真实视频的描述生成更多文本提示，用目标技术生成新的T2V视频；
  - 控制变量：新生成的样本需保持与原数据集一致的分辨率、帧率（如SVD按1024×576、10 FPS生成），确保数据分布统一。

##### 2. 匹配“适配目标技术的真实视频”样本
原项目中真实视频与伪造视频的“生成源关联”是提升检测难度的关键（3.4节），改造时需强化这一关联：
- 若目标技术是I2V（如SVD）：真实视频需选自与生成图像同源的数据集（如HD-VG130M，因SVD的原样本基于该数据集图像生成），避免因场景差异导致模型“靠内容而非技术特征检测”；
- 若目标技术是T2V（如Pika）：真实视频需与T2V的文本提示描述场景一致（如原Pika样本基于VidProM的提示生成，真实视频需选自包含相同场景的Vript）。

##### 3. 优化语义标签：增加“目标技术相关的专项标签”
原项目的语义标签（物体、动作、位置）可辅助场景分析（3.2节、4.5节），改造时需补充与目标技术相关的标签，例如：
- 若目标技术是SVD：补充“图像输入相似度”标签（生成视频与输入图像的视觉相似度），因SVD是I2V技术，该标签可反映技术特性对检测的影响；
- 若目标技术存在“帧间模糊”缺陷：补充“模糊程度”标签（如低/中/高），用于分析缺陷与检测准确率的关联。

##### 4. 调整训练集与测试集划分：强化“目标技术的泛化性验证”
原项目“跨源跨生成器”的划分逻辑（3.4节）需调整为“针对目标技术的泛化性”：
- 训练集：包含“目标技术的部分样本+其他生成器的样本+适配的真实视频”，避免模型仅过拟合目标技术的单一特征；
- 测试集：包含“目标技术的剩余样本（未在训练集中出现的生成源/提示）+适配的真实视频”，验证模型对“未知提示/输入”的目标技术视频的检测能力（贴近真实应用场景，因实际中无法预知目标技术的生成输入）。


#### 三、第三步：模型优化：针对目标技术的特性调整检测模型
原项目选用13种主流视频模型（CNN+Transformer）作为基准（4.2节），改造时需基于目标技术的特征“选择适配模型+优化模型结构”，具体如下：
##### 1. 模型选择：优先适配目标技术的“关键特征维度”
参考原项目中模型对不同生成器的检测表现（Table 5、Table 8），选择对目标技术特征敏感的模型：
- 若目标技术的核心差异在** temporal 特征**（如帧率低、帧间不连贯，如CogVideo）：优先选择强时空建模能力的模型，如VideoSwin（原项目中对帧间关系捕捉较好，Table 5中Top1=67.27%）、MViT V2（原项目最优模型，Top1=79.90%），或CNN类的TSM（ temporal shift 模块，原项目Top1=76.40%）；
- 若目标技术的核心差异在** spatial 特征**（如分辨率低、细节模糊，如Modelscope）：优先选择强空间特征提取的模型，如X3D（原项目CNN类中表现优异，Top1=77.09%）、I3D（对空间细节敏感，Table 5中对部分生成器检测准确率高）。

##### 2. 模型结构优化：增强对目标技术“独特缺陷”的捕捉
基于目标技术的缺陷（如帧间不一致、细节失真），调整模型结构：
- 若目标技术存在“帧间一致性差”：在模型中增加“帧间差异注意力模块”，参考原项目中DeCoF模型“基于帧一致性检测”的思路（3.2节），强化模型对帧间异常的捕捉；
- 若目标技术存在“特定频率域缺陷”（如生成视频在高频段缺乏细节）：在模型输入层增加“频率域特征提取分支”，参考原项目中F3Net“频率域线索挖掘”的思路（4.3节），提升对高频缺陷的敏感度。

##### 3. 训练策略调整：针对目标技术的样本分布优化
- 难例挖掘：参考原项目“硬案例分析”（4.4节），在训练中优先采样目标技术的“难检测样本”（如SVD的植物类视频，Table 7中硬案例比例高），提升模型对难点的识别能力；
- 损失函数调整：若目标技术的样本存在“类别不平衡”（如真实视频与目标技术视频数量差异大），采用加权交叉熵损失，权重基于样本比例设定（原项目未提及但可基于其数据平衡思路补充）。


#### 四、第四步：适配检测任务与评估体系：聚焦目标技术的检测效果
原项目的检测任务（跨源跨生成器、特定场景）（4.2节、4.5节）需改造为“针对目标技术的专项任务”，评估指标需补充技术相关维度：
##### 1. 核心检测任务设计
- 任务1：目标技术视频vs真实视频（二分类）：核心任务，验证模型对目标技术的基础检测能力，输出Top1准确率（参考原项目核心指标）；
- 任务2：目标技术视频vs其他生成技术视频（多分类）：验证模型对目标技术的“特异性检测能力”，避免模型将其他技术误判为目标技术；
- 任务3：特定场景下的目标技术检测（如植物类、车辆类）：参考原项目特定场景任务（4.5节），分析场景对目标技术检测难度的影响，输出不同场景下的Top1准确率（如目标技术在植物类场景的检测准确率）。

##### 2. 评估指标补充：增加目标技术相关的专项指标
除原项目的Top1准确率（4.2节），补充以下指标：
- 若目标技术是I2V：增加“输入图像无关性检测率”——即模型对“生成视频与输入图像差异大”的目标技术视频的检测准确率，反映模型是否依赖输入图像而非技术特征；
- 若目标技术存在生成缺陷：增加“缺陷识别率”——即模型正确检测出含缺陷的目标技术视频的比例（如帧模糊缺陷的识别率）；
- 泛化性指标：增加“跨提示/跨输入的检测准确率”——测试集使用训练集未出现的提示/图像生成的目标技术视频，评估模型泛化性（参考原项目跨源逻辑，3.4节）。

##### 3. 对比实验设计：验证改造效果
参考原项目“与其他数据集/模型对比”的思路（4.3节、Table 6），设计两类对比实验：
- 与原GenVidBench模型对比：将改造后的模型与原项目中同基础模型（如VideoSwin）在“目标技术检测”上的准确率对比，验证改造的有效性；
- 与目标技术专属检测模型对比（若存在）：若已有针对目标技术的简单检测方法（如基于帧率阈值），将改造后的模型与之对比，凸显模型的优势。


#### 五、第五步：复用原项目资源，降低改造成本
原项目提供了数据集与代码（摘要中提及https://genvidbench.github.io），改造时可直接复用以下资源，提升效率：
1. **代码框架**：复用MMAction2框架的训练配置（4.1节），仅修改“数据加载模块”（适配新增的目标技术样本与标签）和“模型输出层”（若任务从二分类改为多分类）；
2. **真实视频源**：直接使用原项目的Vript、HD-VG130M真实视频，无需重新采集；
3. **语义标签工具**：复用原项目基于LLM提取语义标签的方法（3.2节、4.2节），仅补充专项标签的提取逻辑；
4. **评估代码**：复用原项目的Top1准确率计算代码，仅新增专项评估指标的计算函数。


通过以上步骤，可在保留GenVidBench原项目优势（跨源设计、语义标签、标准化框架）的基础上，实现对“特定AI生成技术视频”的精准检测，同时确保改造后的方案具备可复现性与泛化性（符合原项目的实验设计逻辑）。


